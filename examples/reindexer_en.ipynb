{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reindexer\n",
    "\n",
    "[Reindexer](https://github.com/Restream/reindexer) is an embeddable, in-memory, document-oriented database with a high-level Query builder interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how to use the Reindexer vector store with LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation and Setup\n",
    "\n",
    "```python\n",
    "%pip install -qU pyreindexer langchain-reindexer langchain_huggingface sentence_transformers\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_reindexer import ReindexerVectorStore\n",
    "from langchain_core.embeddings import FakeEmbeddings\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize embeddings (FakeEmbeddings doesn't require API keys)\n",
    "\n",
    "For production, use OpenAIEmbeddings, HuggingFaceEmbeddings, or other real embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = FakeEmbeddings(size=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = ReindexerVectorStore(\n",
    "    embedding=embeddings,\n",
    "    rx_connector_config={\"dsn\": \"builtin://\"},  # memory load\n",
    "    rx_namespace=\"langchain_docs\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI Embeddings (requires API key)\n",
    "\n",
    "```python\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Requires OPENAI_API_KEY environment variable\n",
    "openai_embeddings = OpenAIEmbeddings()\n",
    "\n",
    "openai_vector_store = ReindexerVectorStore(\n",
    "    embedding=openai_embeddings,\n",
    "    rx_connector_config={\"dsn\": \"builtin:///tmp/openai_db\"},\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connection Options\n",
    "\n",
    "Reindexer supports different connection types:\n",
    "\n",
    "- **In-memory builtin**: In-memory database (default)\n",
    "  ```python\n",
    "  rx_connector_config={\"dsn\": \"builtin:///tmp/my_db\"}\n",
    "  ```\n",
    "\n",
    "- **By-path builtin**: by-path database\n",
    "  ```python\n",
    "  rx_connector_config={\"dsn\": \"builtin:///tmp/pyrx\"}\n",
    "  ```\n",
    "\n",
    "- **By-auto-path builtin**: By-auto-path database  at the level of the running file\n",
    "  ```python\n",
    "  rx_connector_config={\"dsn\": \"builtin:///\"}\n",
    "  ```\n",
    "\n",
    "For more information, see [Reindexer Documentation](https://github.com/Restream/reindexer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    Document(\n",
    "        page_content=\"LangChain is a framework for developing applications powered by language models.\",\n",
    "        metadata={\"topic\": \"framework\", \"year\": 2023},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Reindexer is an embeddable, in-memory, document-oriented database.\",\n",
    "        metadata={\"topic\": \"database\", \"year\": 2024},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Vector stores enable semantic search over documents using embeddings.\",\n",
    "        metadata={\"topic\": \"vector_search\", \"year\": 2024},\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 3 documents with IDs: ['edc7651b-3260-4608-83ad-b3cbbc85a2c5', 'fe46e5c8-b240-4184-b829-aa0431a53a4e', '343cbb19-28d6-4492-99c0-599a1bc565f8']\n"
     ]
    }
   ],
   "source": [
    "# Add documents to the vector store\n",
    "ids = vector_store.add_documents(documents)\n",
    "print(f\"Added {len(ids)} documents with IDs: {ids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result 1:\n",
      "Content: LangChain is a framework for developing applications powered by language models.\n",
      "Metadata: {'topic': 'framework', 'year': 2023}\n",
      "\n",
      "Result 2:\n",
      "Content: Reindexer is an embeddable, in-memory, document-oriented database.\n",
      "Metadata: {'topic': 'database', 'year': 2024}\n"
     ]
    }
   ],
   "source": [
    "# Search for similar documents\n",
    "query = \"What is LangChain?\"\n",
    "results = vector_store.similarity_search(query, k=2)\n",
    "\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"\\nResult {i}:\")\n",
    "    print(f\"Content: {doc.page_content}\")\n",
    "    print(f\"Metadata: {doc.metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Search with Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.0157\n",
      "Content: Vector stores enable semantic search over documents using embeddings.\n",
      "Metadata: {'topic': 'vector_search', 'year': 2024}\n",
      "\n",
      "Score: 0.0086\n",
      "Content: Reindexer is an embeddable, in-memory, document-oriented database.\n",
      "Metadata: {'topic': 'database', 'year': 2024}\n"
     ]
    }
   ],
   "source": [
    "# Search with similarity scores\n",
    "results_with_scores = vector_store.similarity_search_with_score(query, k=2)\n",
    "\n",
    "for doc, score in results_with_scores:\n",
    "    print(f\"\\nScore: {score:.4f}\")\n",
    "    print(f\"Content: {doc.page_content}\")\n",
    "    print(f\"Metadata: {doc.metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search with Metadata Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content: Reindexer is an embeddable, in-memory, document-oriented database.\n",
      "Metadata: {'topic': 'database', 'year': 2024}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Search with metadata filter\n",
    "filtered_results = vector_store.similarity_search(\n",
    "    query, k=2, filter={\"topic\": \"database\"}\n",
    ")\n",
    "\n",
    "for doc in filtered_results:\n",
    "    print(f\"Content: {doc.page_content}\")\n",
    "    print(f\"Metadata: {doc.metadata}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximal Marginal Relevance (MMR) Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content: LangChain is a framework for developing applications powered by language models.\n",
      "Metadata: {'topic': 'framework', 'year': 2023}\n",
      "\n",
      "Content: Vector stores enable semantic search over documents using embeddings.\n",
      "Metadata: {'topic': 'vector_search', 'year': 2024}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# MMR search for diverse results\n",
    "mmr_results = vector_store.max_marginal_relevance_search(\n",
    "    query=\"framework\", k=2, fetch_k=3, lambda_mult=0.5\n",
    ")\n",
    "\n",
    "for doc in mmr_results:\n",
    "    print(f\"Content: {doc.page_content}\")\n",
    "    print(f\"Metadata: {doc.metadata}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using as a Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content: Reindexer is an embeddable, in-memory, document-oriented database.\n",
      "\n",
      "Content: LangChain is a framework for developing applications powered by language models.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a retriever from the vector store\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "# Use the retriever\n",
    "docs = retriever.invoke(\"What is a vector store?\")\n",
    "for doc in docs:\n",
    "    print(f\"Content: {doc.page_content}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vector store with custom HNSW parameters\n",
    "custom_vector_store = ReindexerVectorStore(\n",
    "    embedding=FakeEmbeddings(size=1024),  # Use same embedding size\n",
    "    m=32,  # More connections for better recall\n",
    "    ef_construction=400,  # Larger candidate list during construction\n",
    "    multithreading=1,  # Enable multithreaded index construction (0 disables)\n",
    "    rx_connector_config={\"dsn\": \"builtin:///tmp/custom_db\"},\n",
    "    rx_namespace=\"custom_namespace\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and Load Configuration\n",
    "\n",
    "- `builtin://` saves data only in memory; use `save_local()` to export to `reindexer_memory_dump.json` file and `load_local()` to restore.\n",
    "\n",
    "- `builtin:///` automatically creates a directory in the script run directory (`reindexer_storage/<namespace>`) and allows Reindexer to manage files itself.\n",
    "\n",
    "- You can specify a custom path after `builtin:///absolute/path` to load data prepared in another environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save vector store configuration\n",
    "vector_store.save_local(\"./reindexer_config\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load vector store configuration\n",
    "loaded_store = ReindexerVectorStore.load_local(\n",
    "    \"./reindexer_config\", embedding=FakeEmbeddings(size=1024)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result 1:\n",
      "Content: Reindexer is an embeddable, in-memory, document-oriented database.\n",
      "Metadata: {'topic': 'database', 'year': 2024}\n",
      "\n",
      "Result 2:\n",
      "Content: Vector stores enable semantic search over documents using embeddings.\n",
      "Metadata: {'topic': 'vector_search', 'year': 2024}\n"
     ]
    }
   ],
   "source": [
    "# Search for similar documents\n",
    "query = \"What is LangChain?\"\n",
    "results = loaded_store.similarity_search(query, k=2)\n",
    "\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"\\nResult {i}:\")\n",
    "    print(f\"Content: {doc.page_content}\")\n",
    "    print(f\"Metadata: {doc.metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using with Real Embeddings\n",
    "\n",
    "For production use, you can use real embedding models. Here are some examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HuggingFace Embeddings\n",
    "\n",
    "```python\n",
    "# Install HuggingFace embeddings: %pip install langchain-huggingface sentence-transformers\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Use HuggingFace embeddings (works locally, no API key required)\n",
    "hf_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "hf_vector_store = ReindexerVectorStore(\n",
    "    embedding=hf_embeddings,\n",
    "    rx_connector_config={\"dsn\": \"builtin:///tmp/hf_db\"},\n",
    ")\n",
    "\n",
    "# Add documents\n",
    "hf_vector_store.add_documents(documents)\n",
    "\n",
    "# Search\n",
    "results = hf_vector_store.similarity_search(\"What is LangChain?\", k=1)\n",
    "print(results[0].page_content)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rx_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
